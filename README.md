<!-- README.md -->
# End-to-End AI Engineering Bootcamp

This repository contains the code and course materials for the [End-to-End AI Engineering Bootcamp](https://maven.com/swirl-ai/end-to-end-ai-engineering). It is designed to be a comprehensive guide to building production-ready AI applications, featuring a full-stack RAG (Retrieval-Augmented Generation) application and a series of educational notebooks.

## ğŸ—ï¸ Project Architecture

The project is structured as a monorepo containing a modern AI application stack:

*   **Frontend**: A [Streamlit](https://streamlit.io/) chatbot interface (`apps/chatbot_ui`) for interacting with the RAG pipeline.
*   **Backend**: A [FastAPI](https://fastapi.tiangolo.com/) service (`apps/api`) that handles retrieval, generation, and orchestration.
*   **Vector Database**: [Qdrant](https://qdrant.tech/) for storing and searching vector embeddings.
*   **State / Checkpointing**: Postgres (LangGraph checkpoint store).
*   **MCP Servers (optional)**: Two [FastMCP](https://github.com/jlowin/fastmcp) servers for tool calls in Week 4 notebooks:
    *   `apps/items_mcp_server` (items retrieval)
    *   `apps/reviews_mcp_server` (reviews retrieval)
*   **Observability**: Integrated with [LangSmith](https://www.langchain.com/langsmith) for tracing, monitoring, and evaluation.
*   **Package Management**: Uses [uv](https://github.com/astral-sh/uv) for fast and reliable Python package management.

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ api/            # FastAPI backend service
â”‚   â”œâ”€â”€ chatbot_ui/     # Streamlit frontend application
â”‚   â”œâ”€â”€ items_mcp_server/   # MCP server (items tools)
â”‚   â””â”€â”€ reviews_mcp_server/ # MCP server (reviews tools)
â”œâ”€â”€ notebooks/          # Educational notebooks organized by curriculum
â”‚   â”œâ”€â”€ prerequisites/  # Intro to LLM APIs
â”‚   â”œâ”€â”€ week_1/         # RAG foundations: Ingestion, Pipeline, Evals
â”‚   â”œâ”€â”€ week_2/         # Advanced RAG: Hybrid Search, Reranking, Structured Outputs
â”‚   â””â”€â”€ week_4/         # Agents + LangGraph + MCP + streaming
â”œâ”€â”€ docker-compose.yml  # Container orchestration
â”œâ”€â”€ Makefile            # Convenience commands
â”œâ”€â”€ pyproject.toml      # Project dependencies and configuration
â””â”€â”€ env.example         # Template for environment variables
```

## ğŸš€ Getting Started

### Prerequisites

*   [Docker Desktop](https://www.docker.com/products/docker-desktop) installed and running.
*   [uv](https://github.com/astral-sh/uv) installed (`curl -LsSf https://astral.sh/uv/install.sh | sh`).
*   API Keys for:
    *   **OpenAI** (for embeddings and LLMs)
    *   **Cohere** (for reranking)
    *   **LangSmith** (optional but recommended for observability)

### Setup

1.  **Clone the repository:**
    ```bash
    git clone <repo-url>
    cd ai_bootcamp
    ```

2.  **Configure Environment Variables:**
    Copy the example environment file and fill in your API keys.
    ```bash
    cp env.example .env
    ```
    *   Edit `.env` and add your keys (`OPENAI_API_KEY`, `CO_API_KEY`, `LANGSMITH_API_KEY`, etc.).

3.  **Install Dependencies:**
    Use `uv` to sync the project dependencies.
    ```bash
    uv sync
    ```

### Running the Application (Docker)

The easiest way to spin up the entire stack (API, Frontend, Database) is using Docker Compose.

```bash
make run-docker-compose
```

This will build the images and start the services:
*   **Streamlit UI**: [http://localhost:8501](http://localhost:8501)
*   **FastAPI Backend**: [http://localhost:8000](http://localhost:8000) (Docs at [/docs](http://localhost:8000/docs))
*   **RAG streaming endpoint**: `POST http://localhost:8000/rag/` (SSE: `text/event-stream`)
*   **Qdrant Dashboard**: [http://localhost:6333/dashboard](http://localhost:6333/dashboard)
*   **Items MCP server**: `http://localhost:8001/mcp`
*   **Reviews MCP server**: `http://localhost:8002/mcp`

*Note: Qdrant data is persisted in the `./qdrant_storage` directory.*

### Troubleshooting

- **`POST /rag` returns 307 redirect**: the FastAPI route is mounted at `/rag/` (trailing slash). Prefer calling `POST /rag/` directly for streaming clients.
- **Notebooks after code changes**: if you edit helper modules (e.g. `utils.py`), restart the Jupyter kernel or reload the module to avoid import caching.

### Running Notebooks

To explore the course materials interactively:

1.  Activate the virtual environment:
    ```bash
    source .venv/bin/activate
    ```
2.  Start Jupyter Lab (or Notebook):
    ```bash
    uv run jupyter lab
    ```

## ğŸ“š Curriculum Overview

### Week 1: Foundations of RAG
*   **Exploratory Data Analysis**: Understanding the Amazon dataset.
*   **Preprocessing**: Cleaning and preparing data for vectorization.
*   **Pipeline Construction**: Building a basic RAG retrieval and generation flow.
*   **Evaluation**: Creating synthetic datasets and evaluating RAG performance using RAGAS/LangSmith.

### Week 2: Advanced RAG Techniques
*   **Structured Outputs**: Forcing LLMs to return JSON/Pydantic models.
*   **Hybrid Search**: Combining dense vectors with sparse (BM25) keyword search.
*   **Reranking**: Improving retrieval quality with Cohere rerankers.
*   **Prompt Engineering**: Versioning and managing prompts as code.

## ğŸ› ï¸ Development

- **Clean Notebooks**: Strip output from notebooks before committing.
  ```bash
  make clean-notebook-outputs
  ```
- **Run Evaluations**: Execute the retriever evaluation script.
  ```bash
  make run-evals-retriever
  ```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
